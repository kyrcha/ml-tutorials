---
title: "Decision Trees"
output: html_document
---

We will work with the iris dataset.

```{r}
fileURL <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
download.file(fileURL, destfile="breast-cancer-wisconsin.data", method="curl")
# read the data
data <- read.table("breast-cancer-wisconsin.data", na.strings = "?", sep=",")
str(data)
data <- data[,-1]
names(data) <- c("ClumpThickness",
"UniformityCellSize",
"UniformityCellShape",
"MarginalAdhesion",
"SingleEpithelialCellSize",
"BareNuclei",
"BlandChromatin",
"NormalNucleoli",
"Mitoses",
"Class")
data$Class <- factor(data$Class, levels=c(2,4), labels=c("benign", "malignant"))
print(summary(data))
```

Let's split the `iris` dataset into two subsets: training (70%) and validation (30%).

```{r}
set.seed(1234)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- data[ind==1,]
validationData <- data[ind==2,]
```

Now we load the libraries `rpart` and `party`

## rpart

```{r}
?rpart
```


```{r}
library(rpart)
library(rpart.plot)
tree = rpart(Class ~ ., data=trainData, method="class")
entTree = rpart(Class ~ ., data=trainData, method="class", parms=list(split="information"))
print(tree)
plot(tree)
text(tree)
```

or run

```{r}
rpart.plot(tree, extra = 104, nn = TRUE)
rpart.control()
```

  - minsplit : το ελάχιστο πλήθος παρατηρήσεων που πρέπει να έχει ένας κόμβος ώστε να διαχωριστεί
  - minbucket : το ελάχιστο επιτρεπτό πλήθος παρατηρήσεων σε κάθε φύλλο του δένδρου
  - maxdepth : το μέγιστο βάθος του δένδρου
  - cp : παράμετρος που ελέγχει αν το complexity για κάποιο διαχωρισμό είναι επιτρεπτό και τίθεται εμπειρικά (όσο μεγαλώνει η τιμή της, τόσο περισσότερο γίνεται pruning στο δένδρο)

## party

```{r}
library(party)
ctree = ctree(Class ~ ., data=trainData)
# print it
print(ctree)
# visualize it
plot(ctree, type="simple")
```

## Evaluation

```{r}
evaluation <- function(model, data, atype) {
  cat("\nConfusion matrix:\n")
  prediction = predict(model, data, type=atype)
  xtab = table(prediction, data$Class)
  print(xtab)
  cat("\nEvaluation:\n\n")
  accuracy = sum(prediction == data$Class)/length(data$Class)
  precision = xtab[1,1]/sum(xtab[,1])
  recall = xtab[1,1]/sum(xtab[1,])
  f = 2 * (precision * recall) / (precision + recall)
  cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
  cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
  cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
  cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(tree, validationData, "class")
evaluation(entTree, validationData, "class")
evaluation(ctree, validationData, "response")
```

## TODO
# Add theory, add as an issue
Add some with different depth
# Explain differences ctree, rpart
