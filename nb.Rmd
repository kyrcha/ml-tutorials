---
title: "Naive Bayes"
author: "Kyriakos Chatzidimitriou (kyrcha@gmail.com)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    highlight: textmate
    includes:
      after_body: footer.html
    css: style.css
  pdf_document:
    includes:
      before_body: head.tex
      after_body: footer.tex
---

## Introduction

The Naive Bayes classifier is included in the `e1071` package. We will also load the `MLmetrics` and `ROCR` packages in order to use their functions for evaluation caclulations and ROC curves.

```{r}
library(e1071)
library(MLmetrics)
library(ROCR)
```
For this example we are going to use the [Breast Cancer Wisconsin (Original) Data Set](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29). The steps for loading and splitting the dataset to training and validation are the same as in the [decision trees notes](dt.html)

```{r include=FALSE}
# download the dataset
fileURL <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
download.file(fileURL, destfile="breast-cancer-wisconsin.data", method="curl")
# read the data
data <- read.table("breast-cancer-wisconsin.data", na.strings = "?", sep=",")
# remove the id column
data <- data[,-1]
# put names in the columns (attributes)
names(data) <- c("ClumpThickness",
"UniformityCellSize",
"UniformityCellShape",
"MarginalAdhesion",
"SingleEpithelialCellSize",
"BareNuclei",
"BlandChromatin",
"NormalNucleoli",
"Mitoses",
"Class")
# make the class a factor
data$Class <- factor(data$Class, levels=c(2,4), labels=c("benign", "malignant"))
# set the seed
set.seed(1234)
# split the dataset
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- data[ind==1,]
validationData <- data[ind==2,]
```

## Training

Next we construct the classifier.

```{r}
model <- naiveBayes(Class ~ ., data=trainData)
print(model)
```

The result includes the following components:

  - apriori: Class distribution for the dependent variable.
  - tables: A list of tables, one for each predictor variable. For each categorical variable a table giving, for each attribute level, the conditional probabilities given the target class. For each numeric variable, a table giving, for each target class, mean and standard deviation of the (sub-)variable.
  
In our case because we have numeric attributes the mean and standard deviations are shown.

## Prediction

Given the model we start making predictions

```{r predict}
predict(model, validationData)
# To get the probabilities and not only the class.
predict(model, validationData, type="raw")
```

## Evaluation

To evaluate the model we use the `MLmetrics` and the `ROCR` packages.

```{r}
ytest <- validationData$Class
pred <- predict(model, validationData)
pred_prob <- predict(model, validationData, type="raw")
ConfusionMatrix(pred, ytest)
Accuracy(pred, ytest)
Precision(ytest, pred)
Recall(ytest, pred)
F1_Score(ytest, pred)
```

### ROC curves

Given the results of the classifier as probabilities we can calculate the true positive rate (TPR) and the false positive rate (FPR) with the commands:

```{r}
pred_obj = ROCR::prediction(pred_prob[, 2], ytest, label.ordering=c("benign", "malignant"))
ROCcurve <- performance(pred_obj, "tpr", "fpr")
```

and we can plot the ROC curve with the commands:

```{r}
plot(ROCcurve, col = "blue")
abline(0,1, col = "grey")
```

In statistics, a [receiver operating characteristic (ROC), or ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic), is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings

An ROC curve demonstrates several things:

  - It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).
  - The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the classifier.
  - The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.
  - The slope of the tangent line at a cutpoint gives the likelihood ratio (LR) for that value of the test. 
  - The area under the curve is a measure of text accuracy. This is discussed further in the next section.

Accuracy is measured by the area under the ROC curve. An area of 1 represents a perfect test; an area of .5 represents a worthless test. A rough guide for classifying the accuracy of a diagnostic test is the traditional academic point system:

.90-1 = excellent (A)
.80-.90 = good (B)
.70-.80 = fair (C)
.60-.70 = poor (D)
.50-.60 = fail (F)

Reference: [http://gim.unmc.edu/dxtests/ROC1.htm](http://gim.unmc.edu/dxtests/ROC1.htm)
