<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Kyriakos Chatzidimitriou" />


<title>k-Nearest Neighbors</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/textmate.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ML & R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="r.html">R</a>
</li>
<li>
  <a href="classification.html">Classification</a>
</li>
<li>
  <a href="uc.html">Clustering</a>
</li>
<li>
  <a href="data-eng.html">Data Engineering</a>
</li>
<li>
  <a href="finder.html">Finder</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">k-Nearest Neighbors</h1>
<h4 class="author"><em>Kyriakos Chatzidimitriou</em></h4>
<h4 class="date"><em>November 7, 2016</em></h4>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The goal of this notebook is to introduce the k-Nearest Neighbors instance-based learning model in R using the class package. For this example we are going to use the <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29">Breast Cancer Wisconsin (Original) Data Set</a>. The steps for loading and splitting the dataset to training and validation are the same as in the <a href="dt.html">decision trees notes</a></p>
</div>
<div id="training-prediction" class="section level2">
<h2>Training &amp; Prediction</h2>
<p>There is a kNN algorithm in the class package.</p>
<pre class="r"><code>library(class)</code></pre>
<p>Because all the attributes are between 1 and 10 there is no need to do normalization between 0 and 1, since no attribute will dominate the others in the distance calculation of kNN. Because kNN accepts the training and testing datasets without the target column, which puts in a 3rd argument, we are going to do some data manipulation to have the data the way the <code>knn</code> function likes them (look the manual with ?knn). Also, because no missing values are allowed in kNN, let???s remove those too.</p>
<pre class="r"><code>trainData &lt;- trainData[complete.cases(trainData),]
validationData &lt;- validationData[complete.cases(validationData),]
trainDataX &lt;- trainData[,-ncol(trainData)]
trainDataY &lt;- trainData$Class
validationDataX &lt;- validationData[,-ncol(trainData)]
validationDataY &lt;- validationData$Class</code></pre>
<p>Lets predict, since there is no need to training when using kNN. The training instances are the model.</p>
<pre class="r"><code>prediction = knn(trainDataX, validationDataX, trainDataY, k = 1)</code></pre>
<p>You can play with the values of k to look for a better model.</p>
</div>
<div id="evaluation" class="section level2">
<h2>Evaluation</h2>
<p>Make the predictions for the validation dataset and print the confusion matrix:</p>
<pre class="r"><code>cat(&quot;Confusion matrix:\n&quot;)</code></pre>
<pre><code>## Confusion matrix:</code></pre>
<pre class="r"><code>xtab = table(prediction, validationData$Class)
print(xtab)</code></pre>
<pre><code>##            
## prediction  benign malignant
##   benign        99         4
##   malignant      6        60</code></pre>
<pre class="r"><code>cat(&quot;\nEvaluation:\n\n&quot;)</code></pre>
<pre><code>## 
## Evaluation:</code></pre>
<pre class="r"><code>accuracy = sum(prediction == validationData$Class)/length(validationData$Class)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste(&quot;Accuracy:\t&quot;, format(accuracy, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Accuracy:     0.94</code></pre>
<pre class="r"><code>cat(paste(&quot;Precision:\t&quot;, format(precision, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Precision:    0.94</code></pre>
<pre class="r"><code>cat(paste(&quot;Recall:\t\t&quot;, format(recall, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Recall:       0.96</code></pre>
<pre class="r"><code>cat(paste(&quot;F-measure:\t&quot;, format(f, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## F-measure:    0.95</code></pre>
<div id="cross-validation" class="section level3">
<h3>Cross Validation</h3>
<p>For performing cross-validation we will use the <code>caret</code> package. <a href="https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf">Here</a> you can find a quick guide to caret.</p>
<div id="split-data-in-two-groups" class="section level4">
<h4>Split data in two groups</h4>
<p>The function <code>createDataParitiion</code> does a stratified random split of the data. Similar to what we did above ourselves (not stratified though). Then we will use the <code>train</code> function to build the kNN model.</p>
<pre class="r"><code>library(caret)
library(mlbench)
data(Sonar)
set.seed(107)
inTrain &lt;- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
training &lt;- Sonar[ inTrain,]
testing &lt;- Sonar[-inTrain,]
nrow(training)</code></pre>
<pre><code>## [1] 157</code></pre>
<pre class="r"><code>nrow(testing)</code></pre>
<pre><code>## [1] 51</code></pre>
<pre class="r"><code>kNNFit &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 157, 157, 157, 157, 157, 157, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy      Kappa       
##   5  0.7491043869  0.4919467330
##   7  0.7484500782  0.4889732990
##   9  0.7458334327  0.4834792808
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<p>We can also search for the best <em>k</em> value given the training dataset.</p>
<pre class="r"><code>kNNFit1 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit1)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 157, 157, 157, 157, 157, 157, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy      Kappa       
##    5  0.7519389199  0.4939164004
##    7  0.7322398739  0.4542330043
##    9  0.7319090223  0.4535786306
##   11  0.7194588084  0.4291184551
##   13  0.7122780236  0.4179367919
##   15  0.7023942713  0.3983258616
##   17  0.6944155541  0.3789999830
##   19  0.6906744445  0.3745708065
##   21  0.6853735800  0.3624348211
##   23  0.6858047028  0.3634749289
##   25  0.6857579586  0.3637183181
##   27  0.6766955520  0.3468448714
##   29  0.6673992283  0.3300590538
##   31  0.6747241970  0.3454881537
##   33  0.6760501728  0.3493650566
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<p>To create a 10-fold cross-validation based search of <em>k</em>, repeated 3 times we have to use the function <code>trainControl</code>:</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 3)
kNNFit2 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                trControl = ctrl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit2)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 141, 141, 140, 141, 142, 142, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy      Kappa       
##    5  0.7878758170  0.5687596395
##    7  0.7792647059  0.5498863582
##    9  0.7725816993  0.5355200743
##   11  0.7598202614  0.5104632130
##   13  0.7289379085  0.4461774405
##   15  0.6903267974  0.3671680535
##   17  0.6878104575  0.3605085405
##   19  0.6956372549  0.3761905368
##   21  0.6977532680  0.3821736209
##   23  0.6920098039  0.3717321737
##   25  0.7082924837  0.4060241541
##   27  0.7039869281  0.3964805263
##   29  0.7295424837  0.4482189256
##   31  0.7170261438  0.4242971412
##   33  0.7294035948  0.4494975698
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<pre class="r"><code>plot(kNNFit2)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>For predictions:</p>
<pre class="r"><code>knnPredict &lt;- predict(kNNFit2, newdata = testing )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, testing$Class )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 25  9
##          R  2 15
##                                                 
##                Accuracy : 0.7843137             
##                  95% CI : (0.6467859, 0.8871094)
##     No Information Rate : 0.5294118             
##     P-Value [Acc &gt; NIR] : 0.0001502273          
##                                                 
##                   Kappa : 0.56                  
##  Mcnemar&#39;s Test P-Value : 0.0704404293          
##                                                 
##             Sensitivity : 0.9259259             
##             Specificity : 0.6250000             
##          Pos Pred Value : 0.7352941             
##          Neg Pred Value : 0.8823529             
##              Prevalence : 0.5294118             
##          Detection Rate : 0.4901961             
##    Detection Prevalence : 0.6666667             
##       Balanced Accuracy : 0.7754630             
##                                                 
##        &#39;Positive&#39; Class : M                     
## </code></pre>
<p>Adding more information in the output:</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 3, classProbs=TRUE, summaryFunction = twoClassSummary)
kNNFit4 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                trControl = ctrl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>kNNFit4</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 142, 140, 141, 142, 142, 142, ... 
## Resampling results across tuning parameters:
## 
##   k   ROC           Sens          Spec        
##    5  0.8953207672  0.8722222222  0.7011904762
##    7  0.8816013558  0.8722222222  0.6767857143
##    9  0.8748387897  0.8638888889  0.6535714286
##   11  0.8678902116  0.8564814815  0.6321428571
##   13  0.8588004299  0.8324074074  0.6196428571
##   15  0.8485656415  0.8162037037  0.5821428571
##   17  0.8406250000  0.8162037037  0.5779761905
##   19  0.8310474537  0.8166666667  0.5910714286
##   21  0.8233837632  0.8166666667  0.5904761905
##   23  0.8290178571  0.7833333333  0.6053571429
##   25  0.8253885582  0.7990740741  0.6375000000
##   27  0.8177124669  0.7930555556  0.6369047619
##   29  0.8109209656  0.7958333333  0.6458333333
##   31  0.8064856151  0.7768518519  0.6553571429
##   33  0.8054687500  0.7611111111  0.6714285714
## 
## ROC was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<pre class="r"><code>plot(kNNFit4)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>And finally how to use bootstrap with the caret package:</p>
<pre class="r"><code>knnFit5 &lt;- train(Class ~ ., 
                data = training,
                 method = &quot;knn&quot;,
                 preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                 tuneLength = 10, 
                 trControl = trainControl(method = &quot;boot&quot;))
knnPredict2 &lt;- predict(knnFit5, newdata = testing)
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict2, testing$Class)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 25  9
##          R  2 15
##                                                 
##                Accuracy : 0.7843137             
##                  95% CI : (0.6467859, 0.8871094)
##     No Information Rate : 0.5294118             
##     P-Value [Acc &gt; NIR] : 0.0001502273          
##                                                 
##                   Kappa : 0.56                  
##  Mcnemar&#39;s Test P-Value : 0.0704404293          
##                                                 
##             Sensitivity : 0.9259259             
##             Specificity : 0.6250000             
##          Pos Pred Value : 0.7352941             
##          Neg Pred Value : 0.8823529             
##              Prevalence : 0.5294118             
##          Detection Rate : 0.4901961             
##    Detection Prevalence : 0.6666667             
##       Balanced Accuracy : 0.7754630             
##                                                 
##        &#39;Positive&#39; Class : M                     
## </code></pre>
<p>Reference: <a href="http://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html" class="uri">http://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html</a></p>
</div>
</div>
</div>

<h2>Acknowledgements</h2>

<p>The tutorials, besides including the author's original contributions and knowledge of Machine Learning algorithms and R, also include contributions from other material and more specifically: 1) from an introductory R leaflet from the Pattern Recognition course leaflets of the Electrical and Computer Engineering Department of the Aristotle University of Thessaloniki (Fall semester 2016, authors: Themistoklis Diamantopoulos/Michalis Papamichail, professor: Andreas Symeonidis), 2) the leaflet from the course CS545 Machine Learnig (Fall 2008, author and professor: Charles W. Anderson) and 3) <a href="https://software-carpentry.org/">Software Caprentry</a> lessons like <a href="http://swcarpentry.github.io/r-novice-inflammation/">Programming in R</a> from which the inflammation data were also included.</p>

<h2>License</h2>

<p>This work is made available under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution license</a>.</p>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-2782275-11', 'auto');
  ga('send', 'pageview');
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
