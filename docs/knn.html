<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Kyriakos Chatzidimitriou (kyrcha@gmail.com)" />


<title>k-Nearest Neighbors</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ML & R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="r.html">R</a>
</li>
<li>
  <a href="classification.html">Classification</a>
</li>
<li>
  <a href="clustering.html">Clustering</a>
</li>
<li>
  <a href="data-eng.html">Data Engineering</a>
</li>
<li>
  <a href="other.html">Other tasks</a>
</li>
<li>
  <a href="finder.html">Finder</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">k-Nearest Neighbors</h1>
<h4 class="author"><em>Kyriakos Chatzidimitriou (<a href="mailto:kyrcha@gmail.com">kyrcha@gmail.com</a>)</em></h4>
<h4 class="date"><em>23 November, 2017</em></h4>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The goal of this notebook is to introduce the k-Nearest Neighbors instance-based learning model in R using the class package. For this example we are going to use the <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29">Breast Cancer Wisconsin (Original) Data Set</a>. The steps for loading and splitting the dataset to training and validation are the same as in the <a href="dt.html">decision trees notes</a></p>
</div>
<div id="training-prediction" class="section level2">
<h2>Training &amp; Prediction</h2>
<p>There is a kNN algorithm in the class package.</p>
<pre class="r"><code>library(class)</code></pre>
<p>Because all the attributes are between 1 and 10 there is no need to do normalization between 0 and 1, since no attribute will dominate the others in the distance calculation of kNN. Because kNN accepts the training and testing datasets without the target column, which puts in a 3rd argument, we are going to do some data manipulation to have the data the way the <code>knn</code> function likes them (look the manual with ?knn). Also, because no missing values are allowed in kNN, let???s remove those too.</p>
<pre class="r"><code>trainData &lt;- trainData[complete.cases(trainData),]
validationData &lt;- validationData[complete.cases(validationData),]
trainDataX &lt;- trainData[,-ncol(trainData)]
trainDataY &lt;- trainData$Class
validationDataX &lt;- validationData[,-ncol(trainData)]
validationDataY &lt;- validationData$Class</code></pre>
<p>Lets predict, since there is no need to training when using kNN. The training instances are the model.</p>
<pre class="r"><code>prediction = knn(trainDataX, validationDataX, trainDataY, k = 1)</code></pre>
<p>You can play with the values of k to look for a better model.</p>
</div>
<div id="evaluation" class="section level2">
<h2>Evaluation</h2>
<p>Make the predictions for the validation dataset and print the confusion matrix:</p>
<pre class="r"><code>cat(&quot;Confusion matrix:\n&quot;)</code></pre>
<pre><code>## Confusion matrix:</code></pre>
<pre class="r"><code>xtab = table(prediction, validationData$Class)
print(xtab)</code></pre>
<pre><code>##            
## prediction  benign malignant
##   benign        99         4
##   malignant      6        60</code></pre>
<pre class="r"><code>cat(&quot;\nEvaluation:\n\n&quot;)</code></pre>
<pre><code>## 
## Evaluation:</code></pre>
<pre class="r"><code>accuracy = sum(prediction == validationData$Class)/length(validationData$Class)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste(&quot;Accuracy:\t&quot;, format(accuracy, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Accuracy:     0.94</code></pre>
<pre class="r"><code>cat(paste(&quot;Precision:\t&quot;, format(precision, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Precision:    0.94</code></pre>
<pre class="r"><code>cat(paste(&quot;Recall:\t\t&quot;, format(recall, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## Recall:       0.96</code></pre>
<pre class="r"><code>cat(paste(&quot;F-measure:\t&quot;, format(f, digits=2), &quot;\n&quot;,sep=&quot; &quot;))</code></pre>
<pre><code>## F-measure:    0.95</code></pre>
<div id="cross-validation" class="section level3">
<h3>Cross Validation</h3>
<p>For performing cross-validation we will use the <code>caret</code> package. <a href="https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf">Here</a> you can find a quick guide to caret.</p>
<div id="split-data-in-two-groups" class="section level4">
<h4>Split data in two groups</h4>
<p>The function <code>createDataParitiion</code> does a stratified random split of the data. Similar to what we did above ourselves (not stratified though). Then we will use the <code>train</code> function to build the kNN model.</p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:MLmetrics&#39;:
## 
##     MAE, RMSE</code></pre>
<pre class="r"><code>library(mlbench)
data(Sonar)
set.seed(107)
inTrain &lt;- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
training &lt;- Sonar[ inTrain,]
testing &lt;- Sonar[-inTrain,]
nrow(training)</code></pre>
<pre><code>## [1] 157</code></pre>
<pre class="r"><code>nrow(testing)</code></pre>
<pre><code>## [1] 51</code></pre>
<pre class="r"><code>kNNFit &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 157, 157, 157, 157, 157, 157, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.7683412  0.5331399
##   7  0.7559100  0.5053696
##   9  0.7378175  0.4715006
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<p>We can also search for the best <em>k</em> value given the training dataset.</p>
<pre class="r"><code>kNNFit1 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit1)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 157, 157, 157, 157, 157, 157, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    5  0.7477453  0.4840235
##    7  0.7189901  0.4225515
##    9  0.7211797  0.4275156
##   11  0.7140987  0.4150135
##   13  0.7031182  0.3932055
##   15  0.7034819  0.3945755
##   17  0.6914316  0.3698916
##   19  0.6855830  0.3588189
##   21  0.6847459  0.3619330
##   23  0.6821917  0.3571894
##   25  0.6626137  0.3186673
##   27  0.6551801  0.3042504
##   29  0.6660760  0.3291024
##   31  0.6643681  0.3273283
##   33  0.6697744  0.3389183
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<p>To create a 10-fold cross-validation based search of <em>k</em>, repeated 3 times we have to use the function <code>trainControl</code>:</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 3)
kNNFit2 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                trControl = ctrl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))
print(kNNFit2)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 142, 142, 142, 142, 141, 142, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    5  0.7820261  0.5548350
##    7  0.7709314  0.5321546
##    9  0.7605147  0.5111479
##   11  0.7504657  0.4924263
##   13  0.7396324  0.4705332
##   15  0.7247386  0.4401370
##   17  0.7058170  0.4006357
##   19  0.7179330  0.4246010
##   21  0.7055229  0.4003491
##   23  0.6930065  0.3779050
##   25  0.6933742  0.3791319
##   27  0.7052451  0.4028364
##   29  0.7163562  0.4277789
##   31  0.7332108  0.4629731
##   33  0.7176389  0.4335254
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<pre class="r"><code>plot(kNNFit2)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>For predictions:</p>
<pre class="r"><code>knnPredict &lt;- predict(kNNFit2, newdata = testing )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, testing$Class )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 25  9
##          R  2 15
##                                           
##                Accuracy : 0.7843          
##                  95% CI : (0.6468, 0.8871)
##     No Information Rate : 0.5294          
##     P-Value [Acc &gt; NIR] : 0.0001502       
##                                           
##                   Kappa : 0.56            
##  Mcnemar&#39;s Test P-Value : 0.0704404       
##                                           
##             Sensitivity : 0.9259          
##             Specificity : 0.6250          
##          Pos Pred Value : 0.7353          
##          Neg Pred Value : 0.8824          
##              Prevalence : 0.5294          
##          Detection Rate : 0.4902          
##    Detection Prevalence : 0.6667          
##       Balanced Accuracy : 0.7755          
##                                           
##        &#39;Positive&#39; Class : M               
## </code></pre>
<p>Adding more information in the output:</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 3, classProbs=TRUE, summaryFunction = twoClassSummary)
kNNFit4 &lt;- train(Class ~ ., 
                data = training,
                method = &quot;knn&quot;,
                tuneLength = 15,
                trControl = ctrl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>kNNFit4</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 142, 142, 142, 141, 141, 141, ... 
## Resampling results across tuning parameters:
## 
##   k   ROC        Sens       Spec     
##    5  0.8846768  0.8773148  0.6988095
##    7  0.8613467  0.8726852  0.6678571
##    9  0.8642361  0.8601852  0.6500000
##   11  0.8592634  0.8518519  0.6494048
##   13  0.8527364  0.8282407  0.6250000
##   15  0.8384011  0.8009259  0.5994048
##   17  0.8389550  0.8004630  0.5904762
##   19  0.8246280  0.8087963  0.5988095
##   21  0.8219783  0.8125000  0.6125000
##   23  0.8125951  0.8055556  0.6125000
##   25  0.8139716  0.7898148  0.6351190
##   27  0.8118676  0.7893519  0.6535714
##   29  0.8090112  0.7842593  0.6577381
##   31  0.8090939  0.7958333  0.6625000
##   33  0.7983300  0.7606481  0.6815476
## 
## ROC was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.</code></pre>
<pre class="r"><code>plot(kNNFit4)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>And finally how to use bootstrap with the caret package:</p>
<pre class="r"><code>knnFit5 &lt;- train(Class ~ ., 
                data = training,
                 method = &quot;knn&quot;,
                 preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                 tuneLength = 10, 
                 trControl = trainControl(method = &quot;boot&quot;))
knnPredict2 &lt;- predict(knnFit5, newdata = testing)
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict2, testing$Class)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 25  9
##          R  2 15
##                                           
##                Accuracy : 0.7843          
##                  95% CI : (0.6468, 0.8871)
##     No Information Rate : 0.5294          
##     P-Value [Acc &gt; NIR] : 0.0001502       
##                                           
##                   Kappa : 0.56            
##  Mcnemar&#39;s Test P-Value : 0.0704404       
##                                           
##             Sensitivity : 0.9259          
##             Specificity : 0.6250          
##          Pos Pred Value : 0.7353          
##          Neg Pred Value : 0.8824          
##              Prevalence : 0.5294          
##          Detection Rate : 0.4902          
##    Detection Prevalence : 0.6667          
##       Balanced Accuracy : 0.7755          
##                                           
##        &#39;Positive&#39; Class : M               
## </code></pre>
<p>Reference: <a href="http://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html" class="uri">http://rstudio-pubs-static.s3.amazonaws.com/16444_caf85a306d564eb490eebdbaf0072df2.html</a></p>
</div>
</div>
</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-2782275-11', 'auto');
  ga('send', 'pageview');
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
