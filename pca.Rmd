---
title: "Principal Components Analysis"
author: "Kyriakos Chatzidimitriou"
date: "November 28, 2016"
output: html_document
---


## Raw PCA

### Step 1: Data

```{r}
d <- c(2.5, 2.4, 0.5, 0.7, 2.2, 2.9, 1.9, 2.2, 3.1, 3.0, 2.3, 
       2.7, 2, 1.6, 1, 1.1, 1.5, 1.6, 1.1, 0.9)
data <- matrix(d, ncol=2, byrow = T)
plot(data, xlab="x1", ylab="x2")
```

### Step 2: Subtract the mean

We subtract the mean of each attribute (x1, x2) from the respective values (centering the data) using the function `scale`.

```{r}
data_norm <- scale(data, scale=F)
plot(data_norm, xlab="x1", ylab="x2")
```

### Step 3: Calculate the covariance matrix

```{r}
S <- cov(data_norm)
print(S)
```

### Step 4: Computer the eigenvectors of the covariance matrix

```{r}
udv <- svd(S)
print(udv)
# asp=1 keep the aspect ratio to 1 so that the eigenvectors are perpendicular
plot(data_norm, asp=1, xlab="x1", ylab="x2")
arrows(0,0,udv$u[1,1],udv$u[2,1], lwd=1)
arrows(0,0,udv$u[1,2],udv$u[2,2], lwd=0.5)
```

### Step 5: Choosing components


```{r}
barplot(udv$d)
print(cumsum(udv$d)/sum(udv$d))
```

We can see that the 1st component accounts for more than 95% of the variance.



```{r}
data_new <- t(udv$u[,1,drop=FALSE]) %*% t(data_norm)
data_new
plot(data_new,data_new,asp=1,xlab="x1", ylab="x2")
```

## PCA using an R function

```{r}
# download the file
#fileURL <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
#download.file(fileURL, destfile="breast-cancer-wisconsin.data", method="curl")
# read the data
data <- read.table('breast-cancer-wisconsin.data', na.strings = "?", sep=",")
data <- data[,-1]
names(data) <- c("ClumpThickness", 
                "UniformityCellSize", 
                "UniformityCellShape", 
                "MarginalAdhesion",
                "SingleEpithelialCellSize",
                "BareNuclei",
                "BlandChromatin",
                "NormalNucleoli",
                "Mitoses",
                "Class")
data$Class <- factor(data$Class, levels=c(2,4), labels=c("benign", "malignant"))
set.seed(1234)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- data[ind==1,]
validationData <- data[ind==2,]

library(class)
library(stats)

trainData <- trainData[complete.cases(trainData),]
validationData <- validationData[complete.cases(validationData),]

trainDataX <- trainData[,-ncol(trainData)]
logTrainDataX <- log(trainDataX)
train.pca <- prcomp(logTrainDataX, center = TRUE, scale. = TRUE)
summary(train.pca)
```

```{r}
# From the summary, you can see that with 7 variables I can explain more than 95%
# of the variance
trainDataY <- trainData$Class
validationDataX <- validationData[,-ncol(trainData)]
# Let's also transform the validation data
logValidationDataX <- log(validationDataX)
validation.pca <- predict(train.pca, newdata=logValidationDataX)
validationDataY <- validationData$Class

# no pca prediction
#prediction = knn(trainDataX, validationDataX, trainDataY, k = 3)
# So let's predict using only the 7 principal components
prediction = knn(train.pca$x[,1:2], validation.pca[,1:2], trainDataY, k = 3)

cat("Confusion matrix:\n")
xtab = table(prediction, validationData$Class)
print(xtab)
cat("\nEvaluation:\n\n")
accuracy = sum(prediction == validationData$Class)/length(validationData$Class)
precision = xtab[1,1]/sum(xtab[,1])
recall = xtab[1,1]/sum(xtab[1,])
f = 2 * (precision * recall) / (precision + recall)
cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" ")) 
```

```{r}
plot(train.pca$x[trainData$Class == 'benign',1:2], col="blue")
points(train.pca$x[trainData$Class == 'malignant',1:2], pch = 3, col="red")

```
